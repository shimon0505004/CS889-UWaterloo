@misc{DoshiVelez2017Towards,
  doi = {10.48550/ARXIV.1702.08608},
  
  url = {https://arxiv.org/abs/1702.08608},
  
  author = {Doshi-Velez, Finale and Kim, Been},
  
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards A Rigorous Science of Interpretable Machine Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DBLP:journals/corr/Lipton16a,
  author    = {Zachary Chase Lipton},
  title     = {The Mythos of Model Interpretability},
  journal   = {CoRR},
  volume    = {abs/1606.03490},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.03490},
  eprinttype = {arXiv},
  eprint    = {1606.03490},
  timestamp = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Lipton16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/RibeiroSG16,
  author    = {Marco T{\'{u}}lio Ribeiro and
               Sameer Singh and
               Carlos Guestrin},
  title     = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  journal   = {CoRR},
  volume    = {abs/1602.04938},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.04938},
  eprinttype = {arXiv},
  eprint    = {1602.04938},
  timestamp = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RibeiroSG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}
@inproceedings{niven2019probing,
  title={Probing Neural Network Comprehension of Natural Language Arguments},
  author={Niven, Timothy and Kao, Hung-Yu},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4658--4664},
  year={2019}
}
@INPROCEEDINGS{9582700,
  author={Dunn, Andrew and Inkpen, Diana and Andonie, Răzvan},
  booktitle={2021 25th International Conference Information Visualisation (IV)}, 
  title={Context-Sensitive Visualization of Deep Learning Natural Language Processing Models}, 
  year={2021},
  volume={},
  number={},
  pages={170-175},
  doi={10.1109/IV53921.2021.00035}
  
}
@article{DBLP:journals/corr/abs-2103-15949,
  author    = {Zeyu Yun and
               Yubei Chen and
               Bruno A. Olshausen and
               Yann LeCun},
  title     = {Transformer visualization via dictionary learning: contextualized
               embedding as a linear superposition of transformer factors},
  journal   = {CoRR},
  volume    = {abs/2103.15949},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.15949},
  eprinttype = {arXiv},
  eprint    = {2103.15949},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-15949.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{skrlj-etal-2021-exploring,
    title = "Exploring Neural Language Models via Analysis of Local and Global Self-Attention Spaces",
    author = "{\v{S}}krlj, Bla{\v{z}}  and
      Sheehan, Shane  and
      Er{\v{z}}en, Nika  and
      Robnik-{\v{S}}ikonja, Marko  and
      Luz, Saturnino  and
      Pollak, Senja",
    booktitle = "Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.hackashop-1.11",
    pages = "76--83",
    abstract = "Large pretrained language models using the transformer neural network architecture are becoming a dominant methodology for many natural language processing tasks, such as question answering, text classification, word sense disambiguation, text completion and machine translation. Commonly comprising hundreds of millions of parameters, these models offer state-of-the-art performance, but at the expense of interpretability. The attention mechanism is the main component of transformer networks. We present AttViz, a method for exploration of self-attention in transformer networks, which can help in explanation and debugging of the trained models by showing associations between text tokens in an input sequence. We show that existing deep learning pipelines can be explored with AttViz, which offers novel visualizations of the attention heads and their aggregations. We implemented the proposed methods in an online toolkit and an offline library. Using examples from news analysis, we demonstrate how AttViz can be used to inspect and potentially better understand what a model has learned.",
}

@inproceedings{cheng2019robust,
  title={Robust Neural Machine Translation with Doubly Adversarial Inputs},
  author={Cheng, Yong and Jiang, Lu and Macherey, Wolfgang},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4324--4333},
  year={2019}
}
@inproceedings{belinkov2018synthetic,
  title={Synthetic and Natural Noise Both Break Neural Machine Translation},
  author={Belinkov, Yonatan and Bisk, Yonatan},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{lin2020rigourous,
  title={A Rigourous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?},
  author={Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Sun, Le},
  journal={arXiv preprint arXiv:2004.12126},
  year={2020}
}
@inproceedings{mehrabi2020man,
  title={Man is to person as woman is to location: Measuring gender bias in named entity recognition},
  author={Mehrabi, Ninareh and Gowda, Thamme and Morstatter, Fred and Peng, Nanyun and Galstyan, Aram},
  booktitle={Proceedings of the 31st ACM Conference on Hypertext and Social Media},
  pages={231--232},
  year={2020}
}
@inproceedings{li2019findings,
  title={Findings of the First Shared Task on Machine Translation Robustness},
  author={Li, Xian and Michel, Paul and Anastasopoulos, Antonios and Belinkov, Yonatan and Durrani, Nadir and Firat, Orhan and Koehn, Philipp and Neubig, Graham and Pino, Juan and Sajjad, Hassan},
  booktitle={Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)},
  pages={91--102},
  year={2019}
}

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{hassan2018achieving,
  title={Achieving human parity on automatic chinese to english news translation},
  author={Hassan, Hany and Aue, Anthony and Chen, Chang and Chowdhary, Vishal and Clark, Jonathan and Federmann, Christian and Huang, Xuedong and Junczys-Dowmunt, Marcin and Lewis, William and Li, Mu and others},
  journal={arXiv preprint arXiv:1803.05567},
  year={2018}
}

@inproceedings{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={353--355},
  year={2018}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{sun2020adv,
  title={Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT},
  author={Sun, Lichao and Hashimoto, Kazuma and Yin, Wenpeng and Asai, Akari and Li, Jia and Yu, Philip and Xiong, Caiming},
  journal={arXiv preprint arXiv:2003.04985},
  year={2020}
}

@inproceedings{michel2018mtnt,
  title={MTNT: A Testbed for Machine Translation of Noisy Text},
  author={Michel, Paul and Neubig, Graham},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={543--553},
  year={2018}
 }
 
 @inproceedings{cohen2019certified,
  title={Certified Adversarial Robustness via Randomized Smoothing},
  author={Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle={International Conference on Machine Learning},
  pages={1310--1320},
  year={2019}
}

@article{carlini2019evaluating,
  title={On evaluating adversarial robustness},
  author={Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
  journal={arXiv preprint arXiv:1902.06705},
  year={2019}
}

@inproceedings{mccoy2019right,
  title={Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
  author={McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3428--3448},
  year={2019}
}

@inproceedings{rashid2019bilingual,
  title={Bilingual-GAN: A Step Towards Parallel Text Generation},
  author={Rashid, Ahmad and Do-Omri, Alan and Haidar, Md Akmal and Liu, Qun and Rezagholizadeh, Mehdi},
  booktitle={Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation},
  pages={55--64},
  year={2019}
}

@inproceedings{rashid2020unsupervised,
  title={From Unsupervised Machine Translation to Adversarial Text Generation},
  author={Rashid, Ahmad and Do-Omri, Alan and Haidar, Md Akmal and Liu, Qun and Rezagholizadeh, Mehdi},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8194--8198},
  year={2020},
  organization={IEEE}
}

@inproceedings{lample2018phrase,
  title={Phrase-Based \& Neural Unsupervised Machine Translation},
  author={Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc’Aurelio},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={5039--5049},
  year={2018}
}

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@inproceedings{krueger2021out,
  title={Out-of-distribution generalization via risk extrapolation (rex)},
  author={Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Le Priol, Remi and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5815--5826},
  year={2021},
  organization={PMLR}
}

@article{peters2016causal,
  title={Causal inference by using invariant prediction: identification and confidence intervals},
  author={Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
  journal={Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  pages={947--1012},
  year={2016},
  publisher={JSTOR}
}


@book{spirtes2000causation,
  title={Causation, prediction, and search},
  author={Spirtes, Peter and Glymour, Clark N and Scheines, Richard and Heckerman, David},
  year={2000},
  publisher={MIT press}
}

@book{pearl2009causality,
  title={Causality},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge university press}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International Conference on Machine Learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@book{bollen1989structural,
  title={Structural equations with latent variables},
  author={Bollen, Kenneth A},
  volume={210},
  year={1989},
  publisher={John Wiley \& Sons}
}

@inproceedings{desai2020calibration,
  title={Calibration of Pre-trained Transformers},
  author={Desai, Shrey and Durrett, Greg},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={295--302},
  year={2020}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@article{45610,
title	= {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
author	= {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
year	= {2016},
URL	= {http://arxiv.org/abs/1609.08144},
journal	= {CoRR},
volume	= {abs/1609.08144}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{moravvcik2017deepstack,
  title={Deepstack: Expert-level artificial intelligence in heads-up no-limit poker},
  author={Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\`y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  journal={Science},
  volume={356},
  number={6337},
  pages={508--513},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{roberts2021common,
  title={Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans},
  author={Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I and Etmann, Christian and McCague, Cathal and Beer, Lucian and others},
  journal={Nature Machine Intelligence},
  volume={3},
  number={3},
  pages={199--217},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{ghaddar2021context,
  title={Context-aware Adversarial Training for Name Regularity Bias in Named Entity Recognition},
  author={Ghaddar, Abbas and Langlais, Philippe and Rashid, Ahmad and Rezagholizadeh, Mehdi},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={586--604},
  year={2021},
  publisher={MIT Press}
}

@article{ghaddarend,
  title={End-to-End Self-Debiasing Framework for Robust NLU Training},
  author={Ghaddar, Abbas and Langlais, Philippe and Rezagholizadeh, Mehdi and Rashid, Ahmad}
}

@inproceedings{rashid-etal-2021-mate,
    title = "{MATE}-{KD}: Masked Adversarial {TE}xt, a Companion to Knowledge Distillation",
    author = "Rashid, Ahmad  and
      Lioutas, Vasileios  and
      Rezagholizadeh, Mehdi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.86",
    doi = "10.18653/v1/2021.acl-long.86",
    pages = "1062--1071",
    abstract = "The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP). While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical. We present MATE-KD, a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits. Then using knowledge distillation a student is trained on both the original and the perturbed training samples. We evaluate our algorithm, using BERT-based models, on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines. On the GLUE test set our 6 layer RoBERTa based model outperforms BERT-large.",
}

@inproceedings{schott2019towards,
  title={Towards the First Adversarially Robust Neural Network Model on MNIST},
  author={Schott, L and Rauber, J and Bethge, M and Brendel, W},
  booktitle={Seventh International Conference on Learning Representations (ICLR 2019)},
  pages={1--16},
  year={2019}
}

@inproceedings{tramer2020fundamental,
  title={Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations},
  author={Tram{\`e}r, Florian and Behrmann, Jens and Carlini, Nicholas and Papernot, Nicolas and Jacobsen, J{\"o}rn-Henrik},
  booktitle={International Conference on Machine Learning},
  pages={9561--9571},
  year={2020},
  organization={PMLR}
  
}

@inproceedings{scholkopf2012causal,
  title={On causal and anticausal learning},
  author={Sch{\"o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={459--466},
  year={2012}
}

@inproceedings{sanh2020learning,
  title={Learning from others' mistakes: Avoiding dataset biases without modeling them},
  author={Sanh, Victor and Wolf, Thomas and Belinkov, Yonatan and Rush, Alexander M},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized BERT pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{shah2020predictive,
  title={Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview},
  author={Shah, Deven Santosh and Schwartz, H Andrew and Hovy, Dirk},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020},
  organization={Association for Computational Linguistics}
}

@inproceedings{utama2020towards,
  title={Towards Debiasing NLU Models from Unknown Biases},
  author={Utama, Prasetya Ajie and Moosavi, Nafise Sadat and Gurevych, Iryna},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7597--7610},
  year={2020}
}

@article{moosavi2020improving,
  title={Improving Robustness by Augmenting Training Sentences with Predicate-Argument Structures},
  author={Moosavi, Nafise Sadat and de Boer, Marcel and Utama, Prasetya Ajie and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2010.12510},
  year={2020}
}

@inproceedings{schuster2019towards,
  title={Towards Debiasing Fact Verification Models},
  author={Schuster, Tal and Shah, Darsh and Yeo, Yun Jie Serene and Ortiz, Daniel Roberto Filizzola and Santus, Enrico and Barzilay, Regina},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3410--3416},
  year={2019}
}

@inproceedings{zhang2019paws,
  title={PAWS: Paraphrase Adversaries from Word Scrambling},
  author={Zhang, Yuan and Baldridge, Jason and He, Luheng},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1298--1308},
  year={2019}
}

@misc{arpit2017closer,
      title={A Closer Look at Memorization in Deep Networks}, 
      author={Devansh Arpit and Stanisław Jastrzębski and Nicolas Ballas and David Krueger and Emmanuel Bengio and Maxinder S. Kanwal and Tegan Maharaj and Asja Fischer and Aaron Courville and Yoshua Bengio and Simon Lacoste-Julien},
      year={2017},
      eprint={1706.05394},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
      
}
      
@inproceedings{dehghani2018fidelity,
  title={Fidelity-Weighted Learning},
  author={Dehghani, Mostafa and Mehrjou, Arash and Gouws, Stephan and Kamps, Jaap and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{wang2019symmetric,
  title={Symmetric cross entropy for robust learning with noisy labels},
  author={Wang, Yisen and Ma, Xingjun and Chen, Zaiyi and Luo, Yuan and Yi, Jinfeng and Bailey, James},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={322--330},
  year={2019}
}

@article{rogers-etal-2020-primer,
    title = "A Primer in {BERT}ology: What We Know About How {BERT} Works",
    author = "Rogers, Anna  and
      Kovaleva, Olga  and
      Rumshisky, Anna",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.54",
    doi = "10.1162/tacl_a_00349",
    pages = "842--866",
    abstract = "Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.",
}

@article{wang2018bi,
  title={A bi-model based rnn semantic frame parsing model for intent detection and slot filling},
  author={Wang, Yu and Shen, Yilin and Jin, Hongxia},
  journal={arXiv preprint arXiv:1812.10235},
  year={2018}
}

@article{qin2019stack,
  title={A stack-propagation framework with token-level intent detection for spoken language understanding},
  author={Qin, Libo and Che, Wanxiang and Li, Yangming and Wen, Haoyang and Liu, Ting},
  journal={arXiv preprint arXiv:1909.02188},
  year={2019}
}

@article{yamada2020luke,
  title={LUKE: deep contextualized entity representations with entity-aware self-attention},
  author={Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Matsumoto, Yuji},
  journal={arXiv preprint arXiv:2010.01057},
  year={2020}
}

@article{alfonso2021nature,
  title={NATURE: Natural Auxiliary Text Utterances forRealistic Spoken Language Evaluation},
  author={Alfonso-Hermelo, David and Rashid, Ahmad and Ghaddar, Abbas and Langlais, Philippe and Rezagholizadeh, Mehdi},
  year={2021}
}

@inproceedings{rosenfeld2021risks,
  title={The Risks of Invariant Risk Minimization},
  author={Rosenfeld, Elan and Ravikumar, Pradeep and Risteski, Andrej},
  booktitle={International Conference on Learning Representations},
  volume={9},
  year={2021}
}

@inproceedings{gaunt2017differentiable,
  title={Differentiable programs with neural libraries},
  author={Gaunt, Alexander L and Brockschmidt, Marc and Kushman, Nate and Tarlow, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={1213--1222},
  year={2017},
  organization={PMLR}
}

@inproceedings{andreas2016neural,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={39--48},
  year={2016}
}

@inproceedings{bahdanau2018systematic,
  title={Systematic Generalization: What Is Required and Can It Be Learned?},
  author={Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and de Vries, Harm and Courville, Aaron},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{parascandolo2018learning,
  title={Learning independent causal mechanisms},
  author={Parascandolo, Giambattista and Kilbertus, Niki and Rojas-Carulla, Mateo and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Machine Learning},
  pages={4036--4044},
  year={2018},
  organization={PMLR}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{hendrycks2021natural,
  title={Natural adversarial examples},
  author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15262--15271},
  year={2021}
}

@article{barbu2019objectnet,
  title={Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},
  author={Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Danny and Tenenbaum, Joshua and Katz, Boris},
  year={2019}
}

@inproceedings{li2017deeper,
  title={Deeper, broader and artier domain generalization},
  author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5542--5550},
  year={2017}
}

@inproceedings{nie2020adversarial,
  title={Adversarial NLI: A New Benchmark for Natural Language Understanding},
  author={Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4885--4901},
  year={2020}
}

@misc{https://doi.org/10.48550/arxiv.2104.02610,
  doi = {10.48550/ARXIV.2104.02610},
  url = {https://arxiv.org/abs/2104.02610},
  author = {Mahmood, Kaleel and Mahmood, Rigel and van Dijk, Marten},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {On the Robustness of Vision Transformers to Adversarial Examples},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@inproceedings{jain2019attention,
  title={Attention is not Explanation},
  author={Jain, Sarthak and Wallace, Byron C},
  booktitle={NAACL-HLT (1)},
  year={2019}
}

@inproceedings{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5797--5808},
  year={2019}
}

@inproceedings{tenney2019bert,
  title={BERT Rediscovers the Classical NLP Pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019}
}

@article{song2020utilizing,
  title={Utilizing BERT intermediate layers for aspect based sentiment analysis and natural language inference},
  author={Song, Youwei and Wang, Jiahai and Liang, Zhiwei and Liu, Zhiyue and Jiang, Tao},
  journal={arXiv preprint arXiv:2002.04815},
  year={2020}
}

@inproceedings{vig-2019-multiscale,
    title = "A Multiscale Visualization of Attention in the Transformer Model",
    author = "Vig, Jesse",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-3007",
    doi = "10.18653/v1/P19-3007",
    pages = "37--42",
}

@inproceedings{aken2020visbert,
  title={Visbert: Hidden-state visualizations for transformers},
  author={Aken, Betty van and Winter, Benjamin and L{\"o}ser, Alexander and Gers, Felix A},
  booktitle={Companion Proceedings of the Web Conference 2020},
  pages={207--211},
  year={2020}
}

@inproceedings{hoover-etal-2020-exbert,
    title = "ex{BERT}: A Visual Analysis Tool to Explore Learned Representations in {T}ransformer Models",
    author = "Hoover, Benjamin  and
    Strobelt, Hendrik  and
    Gehrmann, Sebastian",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-demos.22",
    pages = "187--196",
}

@article{vskrlj2020attviz,
  title={Attviz: Online exploration of self-attention for transparent neural language modeling},
  author={{\v{S}}krlj, Bla{\v{z}} and Er{\v{z}}en, Nika and Sheehan, Shane and Luz, Saturnino and Robnik-{\v{S}}ikonja, Marko and Pollak, Senja},
  journal={arXiv preprint arXiv:2005.05716},
  year={2020}
}

@inproceedings{kobayashi-etal-2020-attention,
    title = "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
    author = "Kobayashi, Goro  and
      Kuribayashi, Tatsuki  and
      Yokoi, Sho  and
      Inui, Kentaro",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.574",
    doi = "10.18653/v1/2020.emnlp-main.574",
    pages = "7057--7075",
    abstract = "Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",
}

@inproceedings{williams-etal-2018-broad,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1101",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
    abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}

@inproceedings{wang2017bilateral,
  title={Bilateral multi-perspective matching for natural language sentences},
  author={Wang, Zhiguo and Hamza, Wael and Florian, Radu},
  booktitle={Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages={4144--4150},
  year={2017}
}

@inproceedings{yang-etal-2019-paws,
    title = "{PAWS}-{X}: A Cross-lingual Adversarial Dataset for Paraphrase Identification",
    author = "Yang, Yinfei  and
      Zhang, Yuan  and
      Tar, Chris  and
      Baldridge, Jason",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1382",
    doi = "10.18653/v1/D19-1382",
    pages = "3687--3692",
    abstract = "Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23{\%} over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information.",
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{ribeiro-etal-2018-semantically,
    title = "Semantically Equivalent Adversarial Rules for Debugging {NLP} models",
    author = "Ribeiro, Marco Tulio  and
      Singh, Sameer  and
      Guestrin, Carlos",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1079",
    doi = "10.18653/v1/P18-1079",
    pages = "856--865",
    abstract = "Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) {--} semantic-preserving perturbations that induce changes in the model{'}s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) {--} simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",
}

@article{Bluff,
  author    = {Nilaksh Das and
               Haekyu Park and
               Zijie J. Wang and
               Fred Hohman and
               Robert Firstman and
               Emily Rogers and
               Duen Horng Chau},
  title     = {Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural
               Networks},
  journal   = {CoRR},
  volume    = {abs/2009.02608},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.02608},
  eprinttype = {arXiv},
  eprint    = {2009.02608},
  timestamp = {Wed, 16 Sep 2020 15:27:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-02608.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{DetectorDetective,
    author    = {Vellaichamy, Sivapriya and Hull, Matthew and Wang, Zijie J. and Das, Nilaksh and Peng, ShengYun and Park, Haekyu and Chau, Duen Horng (Polo)},
    title     = {DetectorDetective: Investigating the Effects of Adversarial Examples on Object Detectors},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {21484-21491}
}

@article{ACTIVIS,
  author    = {Minsuk Kahng and
               Pierre Y. Andrews and
               Aditya Kalro and
               Duen Horng Chau},
  title     = {ActiVis: Visual Exploration of Industry-Scale Deep Neural Network
               Models},
  journal   = {CoRR},
  volume    = {abs/1704.01942},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.01942},
  eprinttype = {arXiv},
  eprint    = {1704.01942},
  timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KahngAKC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{SEQ2SEQVIS,
      title={Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models}, 
      author={Hendrik Strobelt and Sebastian Gehrmann and Michael Behrisch and Adam Perer and Hanspeter Pfister and Alexander M. Rush},
      year={2018},
      eprint={1804.09299},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Goodfellow2015ExplainingAH,
  title={Explaining and Harnessing Adversarial Examples},
  author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6572}
}
@article{DBLP:journals/corr/KurakinGB16,
  author    = {Alexey Kurakin and
               Ian J. Goodfellow and
               Samy Bengio},
  title     = {Adversarial examples in the physical world},
  journal   = {CoRR},
  volume    = {abs/1607.02533},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.02533},
  archivePrefix = {arXiv},
  eprint    = {1607.02533},
  timestamp = {Mon, 13 Aug 2018 16:48:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KurakinGB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{labacacastro2021universal,
      title={Universal Adversarial Perturbations for Malware}, 
      author={Raphael Labaca-Castro and Luis Muñoz-González and Feargus Pendlebury and Gabi Dreo Rodosek and Fabio Pierazzi and Lorenzo Cavallaro},
      year={2021},
      eprint={2102.06747},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{zhu2020freelb,
      title={FreeLB: Enhanced Adversarial Training for Natural Language Understanding}, 
      author={Chen Zhu and Yu Cheng and Zhe Gan and Siqi Sun and Tom Goldstein and Jingjing Liu},
      year={2020},
      eprint={1909.11764},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{jin2019bert,
  title={Is BERT Really Robust? Natural Language Attack on Text Classification and Entailment},
  author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  journal={arXiv preprint arXiv:1907.11932},
  year={2019}
}

@inproceedings{ghaddar-etal-2021-end,
    title = "End-to-End Self-Debiasing Framework for Robust {NLU} Training",
    author = "Ghaddar, Abbas  and
      Langlais, Phillippe  and
      Rezagholizadeh, Mehdi  and
      Rashid, Ahmad",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.168",
    doi = "10.18653/v1/2021.findings-acl.168",
    pages = "1923--1929",
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{li2021select,
  title={How to Select One Among All? An Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding},
  author={Li, Tianda and Rashid, Ahmad and Jafari, Aref and Sharma, Pranav and Ghodsi, Ali and Rezagholizadeh, Mehdi},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={750--762},
  year={2021}
}

@article{clark2019does,
  author    = {Kevin Clark and
               Urvashi Khandelwal and
               Omer Levy and
               Christopher D. Manning},
  title     = {What Does {BERT} Look At? An Analysis of BERT's Attention},
  journal   = {CoRR},
  volume    = {abs/1906.04341},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.04341},
  eprinttype = {arXiv},
  eprint    = {1906.04341},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-04341.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lalparser,
  author    = {Khalil Mrini and
               Franck Dernoncourt and
               Trung Bui and
               Walter Chang and
               Ndapa Nakashole},
  title     = {Rethinking Self-Attention: An Interpretable Self-Attentive Encoder-Decoder
               Parser},
  journal   = {CoRR},
  volume    = {abs/1911.03875},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.03875},
  eprinttype = {arXiv},
  eprint    = {1911.03875},
  timestamp = {Sun, 01 Dec 2019 20:31:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-03875.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}